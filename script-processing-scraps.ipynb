{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import bisect\n",
    "\n",
    "### Script Settings ###\n",
    "\n",
    "# Modify these to change input files and other parameters.\n",
    "\n",
    "# Input filenames:\n",
    "home_folder = '../../../../'\n",
    "original_script_filename = os.path.join(\n",
    "    home_folder, \n",
    "    'original-scripts/force-awakens/force-awakens-lines.csv'\n",
    ")\n",
    "original_script_text_filename = os.path.join(\n",
    "    home_folder,\n",
    "    'original-scripts/force-awakens/force-awakens-with-scene-numbers.txt'\n",
    ")\n",
    "original_script_markup_filename = os.path.join(\n",
    "    home_folder,\n",
    "    'original-scripts/force-awakens/force-awakens-markup.txt'\n",
    ")\n",
    "fan_work_directory = os.path.join(\n",
    "    home_folder, \n",
    "    'fan-works/force-awakens-fullset/plaintext'\n",
    ")\n",
    "\n",
    "def load_csv_script(filename):\n",
    "    with open(filename) as orig_in:\n",
    "        orig_csv = list(csv.reader(orig_in))[1:]\n",
    "        orig_txt = ' '.join(line.strip() for char, line in orig_csv)\n",
    "        tokens = sp(orig_txt)\n",
    "        \n",
    "        characters = []\n",
    "        char_lines = iter(orig_csv)\n",
    "        char, line = next(char_lines, ('', ''))\n",
    "        \n",
    "        start = 0\n",
    "        for end in range(1, len(tokens)):\n",
    "            tok_line = str(tokens[start:end])\n",
    "            if line == tok_line:\n",
    "                characters.extend([char] * (end - start))\n",
    "                char, line = next(char_lines, ('', ''))\n",
    "                start = end            \n",
    "        return tokens, characters\n",
    "    \n",
    "    \n",
    "def load_txt_script(filename):\n",
    "    with open(filename) as orig_in:\n",
    "        orig_txt = orig_in.read()\n",
    "        #orig_txt = re.sub(r'\\s+', ' ', orig_txt).strip()\n",
    "        return orig_txt\n",
    "    \n",
    "txtscr = load_txt_script(original_script_text_filename)\n",
    "csvscr, chars = load_csv_script(original_script_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_char_list = sorted(set(chars))\n",
    "weird_chars = [c for c in full_char_list if any(c in cc and c != cc for cc in full_char_list)]\n",
    "normal_chars = sorted(set(full_char_list) - set(weird_chars))\n",
    "\n",
    "for c in normal_chars:\n",
    "    txtscr = re.sub(r'^[ ]{{11}}\\s*(?P<character>{})[ ]*$'.format(c),\n",
    "                    'CHARACTER_NAME<<\\g<character>>>',\n",
    "                    txtscr,\n",
    "                    flags=re.MULTILINE)\n",
    "for c in weird_chars:\n",
    "    txtscr = re.sub(r'^[ ]{{11}}\\s*(?P<character>{})[ ]*$'.format(c),\n",
    "                    'CHARACTER_NAME<<\\g<character>>>',\n",
    "                    txtscr,\n",
    "                    flags=re.MULTILINE)\n",
    "\n",
    "txtscr = re.sub(r'^           (?P<line>\\S.*)$',\n",
    "                'LINE<<\\g<line>>>',\n",
    "                txtscr,\n",
    "                flags=re.MULTILINE)\n",
    "\n",
    "txtscr = re.sub(r'\\[(?P<number>\\d+)\\]', \n",
    "                'SCENE_NUMBER<<\\g<number>>>', \n",
    "                txtscr,\n",
    "                flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('testout.txt', 'w', encoding='utf-8') as op:\n",
    "#    op.write(txtscr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_line_rex = re.compile('LINE<<(?P<line>[^>]*)>>')\n",
    "_scene_rex = re.compile('SCENE_NUMBER<<(?P<scene>[^>]*)>>')\n",
    "_char_rex = re.compile('CHARACTER_NAME<<(?P<character>[^>]*)>>')\n",
    "\n",
    "def parse_markup_script(filename):\n",
    "    with open(filename, encoding='utf-8') as ip:\n",
    "        current_scene = None\n",
    "        current_char = None\n",
    "        current_line = None\n",
    "        rows = [['LOWERCASE', 'SPACY_ORTH_ID', 'SCENE', 'CHARACTER']]\n",
    "        for i, line in enumerate(ip):\n",
    "            if _scene_rex.search(line):\n",
    "                current_scene = int(_scene_rex.search(line).group('scene'))\n",
    "            elif _char_rex.search(line):\n",
    "                current_char = _char_rex.search(line).group('character')\n",
    "            elif _line_rex.search(line):\n",
    "                tokens = sp(_line_rex.search(line).group('line'))\n",
    "                for t in tokens:\n",
    "                    # original Spacy lexeme object can be recreated using\n",
    "                    #     spacy.lexeme.Lexeme(sp.vocab, t.orth)\n",
    "                    # where `sp = spacy.load('en')`\n",
    "                    row = [t.lower_, t.lower, current_scene, current_char]\n",
    "                    rows.append(row)\n",
    "    return rows\n",
    "\n",
    "script_rows = parse_markup_script(original_script_markup_filename)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[506, 824, 646, 1060, 522]\n"
     ]
    }
   ],
   "source": [
    "class SubseqFinder(object):\n",
    "    def __init__(self, seq):\n",
    "        self.original_seq = seq\n",
    "        self.index_seq = sorted((s, i) for i, s in enumerate(seq))\n",
    "    \n",
    "    def find(self, sub):\n",
    "        if not sub:\n",
    "            return 0\n",
    "\n",
    "        sub_start_val = sub[0]\n",
    "        candidate_ix = bisect.bisect_left(self.index_seq, (sub_start_val, -1))\n",
    "        if candidate_ix == len(self.index_seq):\n",
    "            return -1\n",
    "        \n",
    "        seq_val, seq_ix = self.index_seq[candidate_ix]\n",
    "        maxlen = len(self.original_seq)\n",
    "        while seq_val == sub_start_val:\n",
    "            for sub_ix, sub_val in enumerate(sub):\n",
    "                seq_ix_next = seq_ix + sub_ix\n",
    "                if seq_ix_next >= maxlen:\n",
    "                    break\n",
    "                    \n",
    "                seq_val = self.original_seq[seq_ix_next]\n",
    "                if seq_val != sub_val:\n",
    "                    break\n",
    "            else:\n",
    "                return seq_ix\n",
    "            candidate_ix += 1\n",
    "            seq_val, seq_ix = self.index_seq[candidate_ix]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def subseq_find(seq, sub):\n",
    "    if not sub:\n",
    "        return True\n",
    "    \n",
    "    for i in range(0, len(seq) - len(sub)):\n",
    "        for j, y in enumerate(sub):\n",
    "            x = seq[i + j]\n",
    "            if y != x:\n",
    "                break\n",
    "        else:\n",
    "            return i\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "script_words = [r[1] for r in script_rows]\n",
    "# words = sp('time ago in a galaxy far, far away...')\n",
    "# words = [t.lower for t in words]\n",
    "words = [506, 3637, 504, 303039, 1694, 533, 520, 628, 679, 767]\n",
    "print(script_words[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_structure = {\n",
    "    'fields': ['FAN_WORK_FILENAME', \n",
    "               'FAN_WORK_MATCH_INDEX', \n",
    "               'FAN_WORK_MATCH_TEXT',\n",
    "               'ORIGINAL_SCRIPT_MATCH_INDEX',\n",
    "               'ORIGINAL_SCRIPT_MATCH_TEXT',\n",
    "               'ORIGINAL_SCRIPT_CHARACTERS',\n",
    "               'MATCH_DISTANCE',\n",
    "               'LEVENSHTEIN_DISTANCE',\n",
    "               'COMBINED_DISTANCE'],\n",
    "    'types': [str, int, str, int, str, str, float, int, float]}\n",
    "\n",
    "with open('match-20k-10gram-20170406.csv') as ip:\n",
    "    rows = list(csv.reader(ip))\n",
    "    records = [rows[0]]\n",
    "    records.extend([[record_structure['types'][i](cell) \n",
    "                     for i, cell in enumerate(row)]\n",
    "                    for row in rows if row[0] != 'FAN_WORK_FILENAME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error!\n",
      "['.What', 'you', 'always', 'do', 'to', 'me', 'it', 'is', 'more']\n",
      "['.', 'What', 'do', 'we', 'do', 'with', 'her', '?', 'Is', 'there']\n",
      "702438\n"
     ]
    }
   ],
   "source": [
    "word_records = [['FAN_WORK_FILENAME', \n",
    "                 'FAN_WORK_MATCH_INDEX', \n",
    "                 'FAN_WORK_WORD',\n",
    "                 'FAN_WORK_ORTH_ID',\n",
    "                 'ORIGINAL_SCRIPT_MATCH_INDEX',\n",
    "                 'ORIGINAL_SCRIPT_WORD',\n",
    "                 'ORIGINAL_SCRIPT_ORTH_ID',\n",
    "                 'ORIGINAL_SCRIPT_CHARACTER',\n",
    "                 'ORIGINAL_SCRIPT_SCENE',\n",
    "                 'BEST_MATCH_DISTANCE',\n",
    "                 'BEST_LEVENSHTEIN_DISTANCE',\n",
    "                 'BEST_COMBINED_DISTANCE']]\n",
    "\n",
    "subseq_find = SubseqFinder(script_words).find\n",
    "for i, r in enumerate(records):\n",
    "    rec_words = [t.lower for t in sp(r[4])]\n",
    "    script_ix = subseq_find(rec_words)\n",
    "    \n",
    "    if script_ix >= 0:\n",
    "        rec_texts = [t.orth_ for t in sp(r[4])]\n",
    "        fan_sp = sp(r[2].replace('—', ' - ').replace('-', ' - ').replace('…', '... '))\n",
    "        fan_words = [t.orth for t in fan_sp]\n",
    "        fan_texts = [t.orth_ for t in fan_sp]\n",
    "        \n",
    "        (FILENAME, \n",
    "         FAN_WORK_MATCH_INDEX, \n",
    "         FAN_WORK_MATCH_TEXT,\n",
    "         ORIGINAL_SCRIPT_MATCH_INDEX,\n",
    "         ORIGINAL_SCRIPT_MATCH_TEXT,\n",
    "         ORIGINAL_SCRIPT_CHARACTERS,\n",
    "         MATCH_DISTANCE,\n",
    "         LEVENSHTEIN_DISTANCE,\n",
    "         COMBINED_DISTANCE) = r\n",
    "        for ngram_ix in range(len(rec_words)):\n",
    "            word_ix = script_ix + ngram_ix\n",
    "            word_row = script_rows[word_ix]\n",
    "            SP_LOWER, SP_ORTH, SCENE, CHAR = word_row\n",
    "            \n",
    "            if ngram_ix >= len(fan_texts) or ngram_ix >= len(rec_texts):\n",
    "                print(\"Error!\")\n",
    "                print(fan_texts)\n",
    "                print(rec_texts)\n",
    "                continue\n",
    "                \n",
    "            word_rec = [FILENAME,\n",
    "                        int(FAN_WORK_MATCH_INDEX) + ngram_ix,\n",
    "                        fan_texts[ngram_ix],\n",
    "                        fan_words[ngram_ix],\n",
    "                        int(word_ix),\n",
    "                        rec_texts[ngram_ix],\n",
    "                        rec_words[ngram_ix],\n",
    "                        CHAR,\n",
    "                        int(SCENE),\n",
    "                        float(MATCH_DISTANCE),\n",
    "                        int(LEVENSHTEIN_DISTANCE),\n",
    "                        float(COMBINED_DISTANCE)\n",
    "                       ]\n",
    "            word_records.append(word_rec)\n",
    "        \n",
    "word_record_dedupe = defaultdict(list)\n",
    "for wr in word_records:\n",
    "    word_record_dedupe[(wr[0], wr[1], wr[4])].append(wr)\n",
    "\n",
    "deduped_rows = []\n",
    "for dupe_key in word_record_dedupe:\n",
    "    dupe = word_record_dedupe[dupe_key]\n",
    "    best_match = min(r[9] for r in dupe)\n",
    "    best_lev = min(r[10] for r in dupe)\n",
    "    best_com = min(r[11] for r in dupe)\n",
    "    new_r = dupe[0]\n",
    "    new_r[9] = best_match\n",
    "    new_r[10] = best_lev\n",
    "    new_r[11] = best_com\n",
    "    deduped_rows.append(new_r)\n",
    "\n",
    "print(len(deduped_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('match-20k-10gram-wordlevel-20170406.csv', 'w', encoding='utf-8') as op:\n",
    "    csv.writer(op).writerows(deduped_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
