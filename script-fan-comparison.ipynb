{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "import pandas\n",
    "import nearpy\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Script Settings ###\n",
    "\n",
    "# Modify these to change input files and other parameters.\n",
    "\n",
    "# Input filenames:\n",
    "home_folder = '../../../../'\n",
    "original_script_filename = os.path.join(\n",
    "    home_folder, \n",
    "    'original-scripts/force-awakens/force-awakens-lines.csv'\n",
    ")\n",
    "fan_work_directory = os.path.join(\n",
    "    home_folder, \n",
    "    'fan-works/force-awakens-fullset/plaintext'\n",
    ")\n",
    "\n",
    "# Set N-Gram window size:\n",
    "window_size = 10\n",
    "\n",
    "# Set cosine distance matching threshold:\n",
    "distance_threshold = 0.25\n",
    "\n",
    "# Set approximate nearest neighbor parameters:\n",
    "number_of_hashes = 15  # Bigger -> slower (linear), more matches\n",
    "hash_dimensions = 14   # Bigger -> faster (???), fewer matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell is commented out until we need Bokeh.\n",
    "\n",
    "#import bokeh.io\n",
    "#from bokeh.io import push_notebook, show, output_notebook\n",
    "#from bokeh.plotting import figure\n",
    "#from bokeh.resources import INLINE\n",
    "#from bokeh.models import Range1d\n",
    "\n",
    "#from time import sleep\n",
    "#output_notebook(resources=INLINE)\n",
    "#sleep(1)                  # Otherwise `Run All` messes things up; \n",
    "#bokeh.io._nb_loaded=True  # see https://github.com/bokeh/bokeh/issues/4987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def mk_vectors(sp_txt):\n",
    "    \"\"\"Given a parsed text in `spacy`'s native format, produce\n",
    "    a sequence of vectors, one per token.\n",
    "    \"\"\"\n",
    "    rows = len(sp_txt)\n",
    "    cols = len(sp_txt[0].vector if rows else 0)\n",
    "    vectors = numpy.empty((rows, cols), dtype=float)\n",
    "    for i, word in enumerate(sp_txt):\n",
    "        if word.has_vector:\n",
    "            vectors[i] = word.vector\n",
    "        else:\n",
    "            # It seems `spacy` doesn't have a pre-trained vector for\n",
    "            # this word. So we do something pretty dumb here to give\n",
    "            # the word a vector that is unique to that word and not\n",
    "            # too similar to other words.\n",
    "            w_str = str(word)\n",
    "            vectors[i] = 0\n",
    "            vectors[i][hash(w_str) % cols] = 1.0\n",
    "            vectors[i][hash(w_str * 2) % cols] = 1.0\n",
    "            vectors[i][hash(w_str * 3) % cols] = 1.0\n",
    "    return vectors\n",
    "\n",
    "def cosine_distance(row_values, col_values):\n",
    "    \"\"\"Calculate the cosine distance between two vectos. Also\n",
    "    accepts matrices and 2-d arrays, and calculates the \n",
    "    distances over the cross product of rows and columns.\n",
    "    \"\"\"\n",
    "    verr_msg = '`cosine_distance` is not defined for {}-dimensional arrays.'\n",
    "    if len(row_values.shape) == 1:\n",
    "        row_values = row_values[None,:]\n",
    "    elif len(row_values.shape) != 2:\n",
    "        raise ValueError(verr_msg.format(len(row_values.shape)))\n",
    "    \n",
    "    if len(col_values.shape) == 1:\n",
    "        col_values = col_values[:,None]\n",
    "    elif len(col_values.shape) != 2:\n",
    "        raise ValueError(verr_msg.format(len(col_values.shape)))\n",
    "\n",
    "    row_norm = (row_values * row_values).sum(axis=1) ** 0.5\n",
    "    row_norm = row_norm[:,None]\n",
    "    \n",
    "    col_norm = (col_values * col_values).sum(axis=0) ** 0.5\n",
    "    col_norm = col_norm[None,:]\n",
    "\n",
    "    result = row_values @ col_values\n",
    "    result /= row_norm\n",
    "    result /= col_norm\n",
    "    return 1 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load original script:\n",
    "\n",
    "def load_txt_script(filename):\n",
    "    with open(filename) as orig_in:\n",
    "        orig_txt = orig_in.read()\n",
    "        orig_txt = re.sub(r'\\s+', ' ', orig_txt).strip()\n",
    "        orig = sp(orig_txt)\n",
    "\n",
    "def load_csv_script(filename):\n",
    "    with open(filename) as orig_in:\n",
    "        orig_csv = list(csv.reader(orig_in))[1:]\n",
    "        orig_txt = ' '.join(line.strip() for char, line in orig_csv)\n",
    "        tokens = sp(orig_txt)\n",
    "        \n",
    "        characters = []\n",
    "        char_lines = iter(orig_csv)\n",
    "        char, line = next(char_lines, ('', ''))\n",
    "        \n",
    "        start = 0\n",
    "        for end in range(1, len(tokens)):\n",
    "            tok_line = str(tokens[start:end])\n",
    "            if line == tok_line:\n",
    "                characters.extend([char] * (end - start))\n",
    "                char, line = next(char_lines, ('', ''))\n",
    "                start = end            \n",
    "        return tokens, characters\n",
    "\n",
    "orig, characters = load_csv_script(original_script_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the ngram vectors using rolling windows. \n",
    "# Variables named `*_win_vectors` contain vectors for\n",
    "# the given input, such that each row is the vector\n",
    "# for a single window. Successive windows overlap\n",
    "# at all words except for the first and last.\n",
    "orig_vectors = mk_vectors(orig)\n",
    "orig_win_vectors = numpy.array([orig_vectors[i:i + window_size, :].ravel()\n",
    "                                for i in range(orig_vectors.shape[0] - window_size + 1)])\n",
    "\n",
    "# Initialize the approximate nearest neighbor search algorithm.\n",
    "# This creates the search \"engine\" and populates its index with\n",
    "# the window-vectors from the original script. We can then pass\n",
    "# over the window-vectors from a fan work, taking each vector\n",
    "# and searching for good matches in the engine's index of script\n",
    "# text.\n",
    "\n",
    "# We could do the search in the opposite direction, storing \n",
    "# fan text in the engine's index, and passing over window-\n",
    "# vectors from the original script, searching for matches in \n",
    "# the index of fan text. Unfortuantely, the quality of the \n",
    "# matches found goes down when you add too many values to the\n",
    "# engine's index.\n",
    "vector_dim = orig_win_vectors.shape[1]\n",
    "hashes = []\n",
    "for i in range(number_of_hashes):\n",
    "    h = nearpy.hashes.RandomBinaryProjections('rbp{}'.format(i),\n",
    "                                              hash_dimensions)\n",
    "    hashes.append(h)\n",
    "\n",
    "engine = nearpy.Engine(vector_dim,\n",
    "                       lshashes=hashes,\n",
    "                       distance=nearpy.distances.CosineDistance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ix, row in enumerate(orig_win_vectors):\n",
    "    engine.store_vector(row, (ix, str(orig[ix: ix + window_size])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_match_count = Counter()\n",
    "\n",
    "# Load fan work:\n",
    "\n",
    "fan_works = os.listdir(fan_work_directory)\n",
    "fan_works = [os.path.join(fan_work_directory, f) \n",
    "             for f in fan_works]\n",
    "random.seed(4815162342)  # This will always generate the same \"random\" sample.\n",
    "random.shuffle(fan_works)\n",
    " \n",
    "records = [['FAN_WORK_FILENAME', \n",
    "            'FAN_WORK_MATCH_INDEX', \n",
    "            'FAN_WORK_MATCH_TEXT',\n",
    "            'ORIGINAL_SCRIPT_MATCH_INDEX',\n",
    "            'ORIGINAL_SCRIPT_MATCH_TEXT',\n",
    "            'ORIGINAL_SCRIPT_CHARACTERS',\n",
    "            'MATCH_DISTANCE']]\n",
    "n_windows_processed = 0\n",
    "for works_processed, fan_filename in enumerate(fan_works[0:500], start=1):\n",
    "    with open(fan_filename) as fan_file:\n",
    "        fan = sp(fan_file.read())\n",
    "    \n",
    "    # Create the fan windows:\n",
    "    fan_vectors = mk_vectors(fan)\n",
    "    fan_win_vectors = numpy.array([fan_vectors[i:i + window_size, :].ravel()\n",
    "                                   for i in range(fan_vectors.shape[0] - window_size + 1)])\n",
    "\n",
    "    for fan_ix, row in enumerate(fan_win_vectors):\n",
    "        n_windows_processed += 1\n",
    "        fast_results = engine.neighbours(row)\n",
    "        fast_results = fast_results[0:1]\n",
    "        fast_results = [(match_ix, match_str, distance) \n",
    "                        for vec, (match_ix, match_str), distance in fast_results \n",
    "                        if distance < distance_threshold]\n",
    "\n",
    "        if n_windows_processed % 100000 == 0:\n",
    "            print('* {} texts processed...'.format(works_processed))\n",
    "            print('* {} fan windows processed...'.format(n_windows_processed))\n",
    "            print('{} matches found'.format(len(records)))\n",
    "\n",
    "        if fast_results:\n",
    "            #print()\n",
    "            #print('----------------')\n",
    "            #print('* Matches found!')\n",
    "            #print('    Fan window    (at index {:>6}):   {}'.format(fan_ix, fan[fan_ix: fan_ix + window_size]))\n",
    "            #print()       \n",
    "            #print(\"* Approximate best matches (fast)\")\n",
    "            for match_ix, match_str, distance in fast_results:\n",
    "                #print('    Script window (at index {:>6}):   {}'.format(match_ix, match_str))\n",
    "                #print('    Cosine distance between windows:   {}'.format(distance))\n",
    "                #print()\n",
    "                \n",
    "                records.append([fan_filename, \n",
    "                                fan_ix,\n",
    "                                str(fan[fan_ix: fan_ix + window_size]),\n",
    "                                match_ix,\n",
    "                                match_str,\n",
    "                                sorted(set(characters[match_ix: match_ix + window_size])), \n",
    "                                distance])\n",
    "                orig_match_count[match_ix] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('match-500-trial.csv', 'w', encoding='utf-8') as out:\n",
    "    wr = csv.writer(out)\n",
    "    wr.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "match_strata = [[r for r in records[1:] if r[-1] >= low and r[-1] < high and r[2]]\n",
    "                for low, high in zip([0, 0.05, 0.10, 0.15, 0.20],\n",
    "                                     [0.05, 0.10, 0.15, 0.20, 0.25])]\n",
    "\n",
    "print([len(m) for m in match_strata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxn = max(orig_match_count.keys())\n",
    "for m in match_strata[::-1]:\n",
    "    c = Counter([r[3] for r in m])\n",
    "    pandas.Series([c[n] for n in range(maxn + 1)]).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Most often imitated portions of the original script.\")\n",
    "print()\n",
    "print(\"The imitated phrase appears between <<brakcets>>.\")\n",
    "print(\"Text outside brackets is provided for context, and \")\n",
    "print(\"you can adjust how much context appears by modifying \")\n",
    "print(\"`context_width` above.\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Size of \n",
    "context_width = 5\n",
    "\n",
    "for ix, n in orig_match_count.most_common(20):\n",
    "    print('{} <<{}>> {}'.format(orig[ix - context_width: ix],\n",
    "                                orig[ix: ix + window_size],\n",
    "                                orig[ix + window_size: ix + window_size + context_width]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
